{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360d2783-988a-4000-9c0e-1f2d346a2ad4",
   "metadata": {},
   "source": [
    "The code for depth estimation using the DPT model (dpt-hybrid-midas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884dda03-3719-4086-92eb-5d881e892cc1",
   "metadata": {},
   "source": [
    "Cell 1: Install required libraries (if not already installed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc898e9-1b67-4a88-afe4-93027f634372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if running locally\n",
    "!pip install transformers   # Hugging Face Transformers library\n",
    "!pip install torch          # PyTorch for tensor manipulations\n",
    "!pip install torchvision    # For image processing tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387cd777-6d2a-4e3f-9817-8470265128db",
   "metadata": {},
   "source": [
    "Cell 2: Suppress warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc18b6-b44f-42de-a352-2eac601a4400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning messages to keep the output clean\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fca84c-328f-42c0-b359-c91c5b11a18b",
   "metadata": {},
   "source": [
    "Cell 3: Import libraries and initialize depth estimation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ed7b4-8196-4c4a-8ec8-4599e934e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for depth estimation and image processing\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the depth estimation pipeline with the Intel DPT Hybrid Midas model.\n",
    "depth_estimator = pipeline(task=\"depth-estimation\", model=\"./models/Intel/dpt-hybrid-midas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f686e-641a-4668-9ff3-d2feb54b0e62",
   "metadata": {},
   "source": [
    "Cell 4: Load and preprocess the input image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c4e48-c596-4669-ba9e-c78a817d81d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input image using PIL and resize it to a specific dimension.\n",
    "# This image will be used for depth estimation.\n",
    "raw_image = Image.open('gradio_tamagochi_vienna.png')  # Load an image file\n",
    "raw_image = raw_image.resize((806, 621))  # Resize the image to match the required input size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8101a-46e4-47b7-9d85-d84629567c7a",
   "metadata": {},
   "source": [
    "Cell 5: Perform depth estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab67233-ac14-4073-aaf0-88aba5a0da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform depth estimation using the pre-trained DPT model.\n",
    "# The model will output a depth map prediction for the input image.\n",
    "output = depth_estimator(raw_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dcc0c8-bbec-48e5-a189-bdd88fa9e5b1",
   "metadata": {},
   "source": [
    "Cell 6: Check the size of the predicted depth output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ecaff-4bc1-4af0-b3a4-3507543116e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the shape of the predicted depth output.\n",
    "# The 'predicted_depth' tensor contains the depth map predicted by the model.\n",
    "print(output[\"predicted_depth\"].shape)  # Print the shape of the predicted depth map\n",
    "\n",
    "# Add an additional dimension to the depth map tensor using unsqueeze for later resizing.\n",
    "print(output[\"predicted_depth\"].unsqueeze(1).shape)  # Add a dimension for resizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8084b4-69d2-4e09-9a53-2c0a27b14806",
   "metadata": {},
   "source": [
    "Cell 7: Resize the predicted depth map to match the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed1dea-e0bb-4554-a6d8-8e8dcc5dcd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the predicted depth map to match the original image size using bicubic interpolation.\n",
    "# 'size=raw_image.size[::-1]' ensures that the depth map is resized to the exact dimensions of the original image.\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "    output[\"predicted_depth\"].unsqueeze(1),  # Add a channel dimension for interpolation\n",
    "    size=raw_image.size[::-1],  # Resize to the original image size (width, height)\n",
    "    mode=\"bicubic\",  # Use bicubic interpolation for better quality resizing\n",
    "    align_corners=False  # Disable alignment of corner pixels for smoother interpolation\n",
    ")\n",
    "\n",
    "# Print the shape of the resized depth map to ensure it matches the input image size.\n",
    "print(prediction.shape)\n",
    "print(raw_image.size[::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85aa6c-8b62-45e6-9c56-12ca2ce0be30",
   "metadata": {},
   "source": [
    "Cell 8: Normalize the depth map to display as an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca9aa5-d0c9-4b05-a29e-dd188d901cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the predicted depth map so that it can be visualized as an image.\n",
    "# We normalize the values between 0 and 255 to map them to pixel intensities.\n",
    "output = prediction.squeeze().numpy()  # Remove unnecessary dimensions and convert to NumPy array\n",
    "formatted = (output * 255 / np.max(output)).astype(\"uint8\")  # Normalize the values to the range [0, 255]\n",
    "\n",
    "# Convert the normalized depth map back into a PIL image for visualization.\n",
    "depth = Image.fromarray(formatted)\n",
    "depth  # Display the depth image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612f1855-0612-42fc-9620-baf8bf17860b",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Model Initialization: We initialize the depth estimation pipeline with the DPT model (dpt-hybrid-midas). This model predicts depth maps for input images.\n",
    "\n",
    "Image Preprocessing: The input image is loaded and resized to fit the model's requirements.\n",
    "\n",
    "Depth Estimation: The model generates a predicted depth map for the input image. This is a 2D array representing the depth of each pixel.\n",
    "\n",
    "Resizing the Output: The predicted depth map is resized to match the dimensions of the original input image using bicubic interpolation.\n",
    "\n",
    "Normalization: The depth map is normalized to pixel values between 0 and 255 so that it can be displayed as an image.\n",
    "\n",
    "Display Depth Map: The normalized depth map is converted into a PIL image and can be visualized.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
