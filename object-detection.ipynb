{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9674cf0-61b1-44c9-a6ad-6910d2f3cf93",
   "metadata": {},
   "source": [
    "Cell 1: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44060692-b78b-4c31-b074-2c874184ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for object detection, text-to-speech, image processing, and Gradio interface\n",
    "import os\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589b83f-59ea-4926-b844-a3b78697d531",
   "metadata": {},
   "source": [
    "Cell 2: Define the object detection pipeline and the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e9e5e6-b516-4661-9465-bb5b02c1fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'od_pipe' is the pre-trained object detection pipeline from Hugging Face Transformers.\n",
    "# The function 'get_pipeline_prediction' processes an input image with the object detection pipeline\n",
    "# and returns the image with detected objects drawn on it.\n",
    "\n",
    "def get_pipeline_prediction(pil_image):\n",
    "    # Run object detection pipeline on the input image\n",
    "    pipeline_output = od_pipe(pil_image)\n",
    "    \n",
    "    # Process and render the detection results in the input image\n",
    "    processed_image = render_results_in_image(pil_image, pipeline_output)\n",
    "    \n",
    "    # Return the image with predicted objects drawn on it\n",
    "    return processed_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0322807-bd80-437f-ba9e-adfbc220c918",
   "metadata": {},
   "source": [
    "Cell 3: Create the Gradio interface for object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6246cca-b0f9-4bb6-acc4-8c4f1c66b4b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PORT1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m demo \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mInterface(\n\u001b[1;32m      6\u001b[0m     fn\u001b[38;5;241m=\u001b[39mget_pipeline_prediction,        \u001b[38;5;66;03m# Function to call for processing\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mImage(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpil\u001b[39m\u001b[38;5;124m\"\u001b[39m),   \u001b[38;5;66;03m# Input type: Image\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     outputs\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mImage(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput image with predicted instances\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpil\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Output type: Image\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# The 'share=True' argument will create a sharable link, making the app accessible online.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# The 'server_port' is set using the environment variable for deployment flexibility.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m demo\u001b[38;5;241m.\u001b[39mlaunch(share\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, server_port\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPORT1\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m<frozen os>:679\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PORT1'"
     ]
    }
   ],
   "source": [
    "# Creating a Gradio interface for the object detection app.\n",
    "# This interface takes an image as input, processes it through the object detection pipeline,\n",
    "# and displays the output image with detected objects.\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=get_pipeline_prediction,        # Function to call for processing\n",
    "    inputs=gr.Image(label=\"Input image\", type=\"pil\"),   # Input type: Image\n",
    "    outputs=gr.Image(label=\"Output image with predicted instances\", type=\"pil\")  # Output type: Image\n",
    ")\n",
    "\n",
    "# The 'share=True' argument will create a sharable link, making the app accessible online.\n",
    "# The 'server_port' is set using the environment variable for deployment flexibility.\n",
    "demo.launch(share=True, server_port=int(os.environ['PORT1']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ccba4-a7df-4708-b91d-dea2b2c865de",
   "metadata": {},
   "source": [
    "Cell 4: Close the Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e79f3e-2ef8-4a2e-ae0f-ff02e5f5d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you are done with the demo, you can close the Gradio interface.\n",
    "demo.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d53668-e93a-4aac-80f5-d54ecd36e92a",
   "metadata": {},
   "source": [
    "Cell 5: Define the text summarization and TTS generation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fc37cb4-2350-41bf-bc93-27879519b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'summarize_predictions_natural_language' is a helper function\n",
    "# that takes the object detection pipeline output and summarizes it in natural language.\n",
    "\n",
    "def generate_narration(pipeline_output):\n",
    "    # Generate a natural language description from the object detection output\n",
    "    text = summarize_predictions_natural_language(pipeline_output)\n",
    "    \n",
    "    # Define the text-to-speech pipeline using the Kakao model for audio generation\n",
    "    tts_pipe = pipeline(\"text-to-speech\", model=\"./models/kakao-enterprise/vits-ljs\")\n",
    "    \n",
    "    # Generate speech from the textual description\n",
    "    narrated_text = tts_pipe(text)\n",
    "    \n",
    "    # Return the narrated text audio\n",
    "    return narrated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf480d09-4c9e-4b1d-b0b8-2fc4cf847c96",
   "metadata": {},
   "source": [
    "Cell 6: Play the generated audio from text-to-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3979b70d-cd4d-4a9d-b862-e8d68e8293cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to play the generated audio using IPython's Audio class\n",
    "# This function accepts the narrated text from the TTS pipeline and plays it in the notebook.\n",
    "\n",
    "from IPython.display import Audio as IPythonAudio\n",
    "\n",
    "def play_generated_audio(narrated_text):\n",
    "    # Play the audio output from the TTS model, with the appropriate sampling rate\n",
    "    return IPythonAudio(narrated_text[\"audio\"][0], rate=narrated_text[\"sampling_rate\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53abcad7-8897-4a2b-87af-74a4052066da",
   "metadata": {},
   "source": [
    "Cell 7: Combine Object Detection and TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a19a8b15-d0c7-4963-be06-2658417f6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining object detection and TTS models. After detecting objects in the image,\n",
    "# the output is summarized in natural language and converted to speech.\n",
    "\n",
    "def detect_and_narrate(pil_image):\n",
    "    # Step 1: Perform object detection\n",
    "    pipeline_output = od_pipe(pil_image)\n",
    "    \n",
    "    # Step 2: Generate the processed image with detection results\n",
    "    processed_image = render_results_in_image(pil_image, pipeline_output)\n",
    "    \n",
    "    # Step 3: Generate a natural language description of the objects detected\n",
    "    narration = generate_narration(pipeline_output)\n",
    "    \n",
    "    # Return the processed image and the narrated audio\n",
    "    return processed_image, narration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaf2fb6-ccbc-4ee5-8f06-05a92e67e2ad",
   "metadata": {},
   "source": [
    "Cell 8: Gradio Interface for Object Detection + TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c37410d-a783-4ead-aeec-5983f527956f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'PORT1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m demo_with_tts \u001b[38;5;241m=\u001b[39m gr\u001b[38;5;241m.\u001b[39mInterface(\n\u001b[1;32m      5\u001b[0m     fn\u001b[38;5;241m=\u001b[39mdetect_and_narrate,  \u001b[38;5;66;03m# Function to call for processing the image and generating narration\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mgr\u001b[38;5;241m.\u001b[39mImage(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpil\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# Input type: Image\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     outputs\u001b[38;5;241m=\u001b[39m[gr\u001b[38;5;241m.\u001b[39mImage(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed image with detected objects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpil\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# Output type: Processed image\u001b[39;00m\n\u001b[1;32m      8\u001b[0m              gr\u001b[38;5;241m.\u001b[39mAudio(label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio narration of objects\u001b[39m\u001b[38;5;124m\"\u001b[39m)]  \u001b[38;5;66;03m# Output type: Audio (narrated description)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Launch the app and provide a shareable link\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m demo_with_tts\u001b[38;5;241m.\u001b[39mlaunch(share\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, server_port\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPORT1\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m<frozen os>:679\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PORT1'"
     ]
    }
   ],
   "source": [
    "# Create a Gradio interface for the combined object detection and TTS pipeline.\n",
    "# The app will return both the processed image and the narrated description as audio.\n",
    "\n",
    "demo_with_tts = gr.Interface(\n",
    "    fn=detect_and_narrate,  # Function to call for processing the image and generating narration\n",
    "    inputs=gr.Image(label=\"Input image\", type=\"pil\"),  # Input type: Image\n",
    "    outputs=[gr.Image(label=\"Processed image with detected objects\", type=\"pil\"),  # Output type: Processed image\n",
    "             gr.Audio(label=\"Audio narration of objects\")]  # Output type: Audio (narrated description)\n",
    ")\n",
    "\n",
    "# Launch the app and provide a shareable link\n",
    "demo_with_tts.launch(share=True, server_port=int(os.environ['PORT1']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee27486-3767-4a72-ab6f-410069d61663",
   "metadata": {},
   "source": [
    "Cell 9: Close the Gradio app when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac25a3-1463-4e09-bbfb-3a5c2911fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After using the Gradio app, it can be closed to free up resources.\n",
    "demo_with_tts.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8e34a-7dc5-433b-936b-ec3265b122e8",
   "metadata": {},
   "source": [
    "Explanation of the process:\n",
    "Object Detection: The get_pipeline_prediction function runs an object detection pipeline on an input image and returns an image with detected objects.\n",
    "\n",
    "Text Summarization: The generate_narration function summarizes the objects detected in the image in natural language.\n",
    "\n",
    "Text-to-Speech: The summarized text is converted into speech using a pre-trained text-to-speech pipeline.\n",
    "\n",
    "Gradio Interface: The demo_with_tts Gradio interface allows users to upload an image, see the detected objects, and hear a narrated description of what is in the image.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
