{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e2792c-b4c3-4e29-8260-bc9679662b5f",
   "metadata": {},
   "source": [
    "Cell 1: Install necessary libraries (if not already installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa2ea1-3b38-4213-b418-e9a61bc8002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if running on a local machine\n",
    "!pip install transformers  # Hugging Face's Transformers library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139063c5-25d5-4100-bb75-7c8c0510ceca",
   "metadata": {},
   "source": [
    "Cell 2: Suppress warning messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d58e75-726a-4662-99ae-ce858cccf51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning messages from the Transformers library to keep the output clean.\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Suppress additional warnings related to the model generation process.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Using the model-agnostic default `max_length`\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49738e67-1989-4b44-837c-a5be8b1b7c95",
   "metadata": {},
   "source": [
    "Cell 3: Load the Visual Question Answering (VQA) model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8efc7-5080-4d72-a4f3-811ac7d0f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BLIP model for visual question answering and the processor to prepare inputs.\n",
    "from transformers import BlipForQuestionAnswering, AutoProcessor\n",
    "\n",
    "# Load the pre-trained BLIP VQA model from the specified path.\n",
    "model = BlipForQuestionAnswering.from_pretrained(\"./models/Salesforce/blip-vqa-base\")\n",
    "\n",
    "# Load the processor that formats the image and the question into inputs for the model.\n",
    "processor = AutoProcessor.from_pretrained(\"./models/Salesforce/blip-vqa-base\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "75dd7d95-5a69-4334-b3ef-08f48f6a3f33",
   "metadata": {},
   "source": [
    "Cell 4: Load and display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546ba79-baa6-440a-943f-b7dcf6dae034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PIL for image loading and processing.\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image from the specified file path. This image will be used for the VQA task.\n",
    "image = Image.open(\"./beach.jpeg\")\n",
    "\n",
    "# Display the image to verify it's correctly loaded.\n",
    "image\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "65da8338-7bdf-480e-a35b-0f02914afc9c",
   "metadata": {},
   "source": [
    "Cell 5: Prepare the question and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4829b8-170f-48f0-84a0-83246a405470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the question you want to ask the model about the image.\n",
    "# Example question: \"how many dogs are in the picture?\"\n",
    "question = \"how many dogs are in the picture?\"\n",
    "\n",
    "# Use the processor to format the image and question as inputs for the model.\n",
    "# The return_tensors=\"pt\" argument ensures that the inputs are returned as PyTorch tensors.\n",
    "inputs = processor(image, question, return_tensors=\"pt\")\n",
    "\n",
    "# Display the processed inputs for verification.\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec5fed5-fab9-4f2d-9283-df018507ba91",
   "metadata": {},
   "source": [
    "Cell 6: Generate the answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571df93-2851-4b42-8838-5bed8abaf05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the formatted inputs through the model to generate an answer to the question.\n",
    "# The model processes both the image and the question.\n",
    "out = model.generate(**inputs)\n",
    "\n",
    "# Display the raw output of the model's answer generation.\n",
    "out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15377f1f-a6d2-4a37-9558-3a1f4c63c02c",
   "metadata": {},
   "source": [
    "Cell 7: Decode and print the answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b2db1-3031-49b8-9ec3-98c762b3b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated output using the processor to convert the tokenized output into a human-readable string.\n",
    "# The 'skip_special_tokens=True' argument removes any special tokens from the output.\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de81d4-7149-45be-86e5-347fab086445",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Model Initialization: The BLIP VQA model and processor are loaded. The model answers questions based on the input image and question.\n",
    "\n",
    "Image Loading: The input image is loaded using PIL and displayed to ensure it was loaded correctly.\n",
    "\n",
    "Question Preparation: The question about the image (e.g., \"how many dogs are in the picture?\") is defined.\n",
    "\n",
    "Input Processing: The processor prepares both the image and the question, formatting them as model-ready inputs, and returns them as PyTorch tensors.\n",
    "\n",
    "Answer Generation: The model processes the inputs and generates an answer based on the image and the question.\n",
    "\n",
    "Answer Decoding: The modelâ€™s output is decoded into a readable string and printed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
