{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f83c8f8-e292-4e5a-84ec-9dec89179921",
   "metadata": {},
   "source": [
    "Cell 1: Install necessary libraries (if not already installed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45a309b7-a0c1-4ce6-ae05-8804f327f592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (4.37.2)\n",
      "Requirement already satisfied: filelock in /Users/riteshgaire/.local/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: torch in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (2.2.0.post100)\n",
      "Requirement already satisfied: filelock in /Users/riteshgaire/.local/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/riteshgaire/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries if running locally\n",
    "!pip install transformers  # Hugging Face Transformers library\n",
    "!pip install torch         # PyTorch for tensor manipulations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0081e0-e9f3-443a-a972-37b0ac709405",
   "metadata": {},
   "source": [
    "Cell 2: Suppress warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e4ca5-9c5d-43b8-aa1f-fe460cb83e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning messages to keep the output clean\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167fe90a-a2d3-4171-be2c-ba0f81348342",
   "metadata": {},
   "source": [
    "Cell 3: Load the model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e17bd-2646-4b38-96b3-b2c2621126b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the BLIP model for image-text matching and the processor to prepare the inputs.\n",
    "from transformers import BlipForImageTextRetrieval, AutoProcessor\n",
    "\n",
    "# Load the pre-trained BLIP image-text retrieval model. This model checks if a given image and text match.\n",
    "model = BlipForImageTextRetrieval.from_pretrained(\"./models/Salesforce/blip-itm-base-coco\")\n",
    "\n",
    "# Load the processor that formats images and text as inputs for the model.\n",
    "processor = AutoProcessor.from_pretrained(\"./models/Salesforce/blip-itm-base-coco\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3cc651-4f67-4186-8206-a679cbc9bc7b",
   "metadata": {},
   "source": [
    "Cell 4: Load the image from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8539c6bc-7534-4dde-b7ef-63778dbbb17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for loading and processing images.\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load an example image from a URL and convert it to RGB format for processing.\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# Display the loaded image to check if it was loaded correctly.\n",
    "raw_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e9741-a386-4967-92d0-abbc963d1288",
   "metadata": {},
   "source": [
    "Cell 5: Test if the image matches the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817a9d1-c2fa-4f78-8496-4135bd59ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text that describes the image. In this case, we are using the description:\n",
    "# \"an image of a woman and a dog on the beach\".\n",
    "text = \"an image of a woman and a dog on the beach\"\n",
    "\n",
    "# Use the processor to format the image and text into model-ready inputs.\n",
    "# The 'return_tensors=\"pt\"' argument ensures that the inputs are returned as PyTorch tensors.\n",
    "inputs = processor(images=raw_image, text=text, return_tensors=\"pt\")\n",
    "\n",
    "# Display the processed inputs for verification.\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5407d22f-ccb7-4722-a1a7-e2c0f9fb89c9",
   "metadata": {},
   "source": [
    "Cell 6: Get image-text matching scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26086f8-b4c2-4a47-89bc-73dc92d422a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the formatted inputs through the model to get image-text matching (ITM) scores.\n",
    "itm_scores = model(**inputs)[0]  # The first element of the model's output contains the scores\n",
    "\n",
    "# Display the raw ITM scores before applying softmax.\n",
    "itm_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828e0ad-4886-46e1-938a-4c4c0ddd10ce",
   "metadata": {},
   "source": [
    "Cell 7: Calculate probabilities using softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708dd0d2-5e0a-402a-b0bd-7403db9487be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyTorch's softmax function to convert the raw ITM scores into probabilities.\n",
    "import torch\n",
    "\n",
    "# Apply the softmax function to the ITM scores to get the probabilities of matching.\n",
    "itm_score = torch.nn.functional.softmax(itm_scores, dim=1)\n",
    "\n",
    "# Display the softmax probabilities for each class (match/no match).\n",
    "itm_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b348d62-ceaa-49e3-a4ea-66743a73bec3",
   "metadata": {},
   "source": [
    "Cell 8: Print the matching probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a07d29-b15b-4861-a2ee-7be79ce53523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image-text matching probability for the \"match\" class is located in itm_score[0][1].\n",
    "# Print the result, formatting the probability to 4 decimal places.\n",
    "print(f\"The image and text are matched with a probability of {itm_score[0][1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72b8c52-8006-4dd2-af4c-369ca56d699b",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Model Initialization: We load the BlipForImageTextRetrieval model and its corresponding processor. The model predicts whether a given image matches a given text description.\n",
    "\n",
    "Image Loading: The input image is loaded from a URL, converted to RGB format, and displayed for inspection.\n",
    "\n",
    "Input Processing: The processor formats the image and text into a format the model understands, returning PyTorch tensors.\n",
    "\n",
    "Model Inference: The model processes the inputs and returns image-text matching scores.\n",
    "\n",
    "Softmax Calculation: We use a softmax function to convert the raw scores into probabilities, indicating the likelihood that the image and text match.\n",
    "\n",
    "Display Probability: The probability that the image matches the text description is printed to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c49c0-75f5-4642-a68d-7c7aab239138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
