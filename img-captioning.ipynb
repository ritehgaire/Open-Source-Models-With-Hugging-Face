{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75bf084a-8d99-4414-8504-b96f6d18f0f1",
   "metadata": {},
   "source": [
    "Cell 1: Install necessary libraries (if not already installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27f383-f390-44e7-b44e-92259fa62bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if running on a local machine\n",
    "!pip install transformers  # Hugging Face's Transformers library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda9f2e-8d6f-437b-8f30-2f619e036c1e",
   "metadata": {},
   "source": [
    "Cell 2: Suppress warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc2e4d-1126-41be-8336-dc629e6b4ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning messages from the Transformers library to keep the output clean.\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Suppress additional specific warnings related to the default max_length parameter in model generation.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Using the model-agnostic default `max_length`\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e024eddd-ab20-4113-b6f2-7f82924152f5",
   "metadata": {},
   "source": [
    "Cell 3: Load the image captioning model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11461b-c704-4c55-9616-872971ae4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import BLIP model for conditional image captioning and the processor to prepare inputs.\n",
    "from transformers import BlipForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# Load the pre-trained BLIP model for image captioning from the specified directory.\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"./models/Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Load the processor that prepares the image and text inputs for the model.\n",
    "processor = AutoProcessor.from_pretrained(\"./models/Salesforce/blip-image-captioning-base\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2511ddf-c959-4975-b58d-61dddbdd8d69",
   "metadata": {},
   "source": [
    "Cell 4: Load and display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e8100-9581-4180-ac88-b2be749baa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PIL for image loading and processing.\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image from the specified file path and display it for verification.\n",
    "image = Image.open(\"./beach.jpeg\")\n",
    "\n",
    "# Display the image to verify it's correctly loaded.\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb89294-0546-4e6c-a072-ead79912bc69",
   "metadata": {},
   "source": [
    "Cell 5: Conditional image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253f4e1-6a47-422c-9d4b-afd187c71d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a conditional prefix text for generating an image caption.\n",
    "# In this case, we use \"a photograph of\" as the starting point for the caption.\n",
    "text = \"a photograph of\"\n",
    "\n",
    "# Use the processor to prepare the image and text as inputs for the model.\n",
    "# The return_tensors=\"pt\" argument ensures that the inputs are returned as PyTorch tensors.\n",
    "inputs = processor(image, text, return_tensors=\"pt\")\n",
    "\n",
    "# Display the processed inputs for verification.\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ad2d74-807c-489c-8849-c15bd3171ebf",
   "metadata": {},
   "source": [
    "Cell 6: Generate the conditional image caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f7a68-25e1-4446-babc-b85f153121a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a caption using the model. The inputs include both the image and the conditional text.\n",
    "out = model.generate(**inputs)\n",
    "\n",
    "# Display the raw output of the model's caption generation.\n",
    "out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38854808-b982-42a2-a259-4a67f56f7ab2",
   "metadata": {},
   "source": [
    "Cell 7: Decode and print the conditional caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ed270-cc0b-40b4-a5c0-f943b5bb393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode the generated output using the processor to convert the tokenized output into a human-readable string.\n",
    "# The 'skip_special_tokens=True' argument removes any special tokens from the output.\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78aa67b-4969-45fe-b4c2-a8f18d3b5845",
   "metadata": {},
   "source": [
    "Cell 8: Unconditional image captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a143c0-cb95-496d-80b2-b50c53ab7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unconditional image captioning, we omit the text prefix and provide only the image as input.\n",
    "# The processor formats the image as input for the model.\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a caption for the image without any text prefix (unconditional captioning).\n",
    "out = model.generate(**inputs)\n",
    "\n",
    "# Decode and print the unconditional caption generated by the model.\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb1b9e-0be4-4046-a5b5-cf62bb8a2ef5",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Model Initialization: The BLIP image captioning model and processor are loaded. The model is used to generate captions based on the input image, either conditionally or unconditionally.\n",
    "\n",
    "Image Loading: The input image is loaded using PIL. This image will be passed to the model for caption generation.\n",
    "\n",
    "Conditional Captioning: A prefix text (\"a photograph of\") is provided to guide the model in generating a caption. The processor prepares the image and text, and the model generates a caption based on both inputs.\n",
    "\n",
    "Unconditional Captioning: In the unconditional captioning, only the image is passed to the model without any prefix text, allowing the model to generate a caption solely based on the image content.\n",
    "\n",
    "Caption Generation: The model generates captions, which are decoded from the tokenized output into human-readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9adb0-6db5-49fe-8308-b8ea09513af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
