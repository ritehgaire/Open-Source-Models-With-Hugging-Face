{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c19f4b8-991e-474d-b8f4-74d9d8696164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules from the transformers and torch libraries\n",
    "from transformers import pipeline  # For using Hugging Face models easily\n",
    "import torch  # To manage the model data types and GPU handling\n",
    "import gc  # To manually free up memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "850c4ce1-fe05-4d3e-b983-9d1b02435656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NLLB translation model from the Hugging Face Model Hub.\n",
    "# Use bfloat16 for GPU (if available) and float32 for CPU.\n",
    "translator = pipeline(\n",
    "    task=\"translation\", \n",
    "    model=\"facebook/nllb-200-distilled-600M\", \n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9aa59e-553e-48d9-aeb0-ecb7cd9386f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text: [{'translation_text': 'Mon chiot est adorable, ton chaton est mignon, son panda est ami, son lama est attentionn√©, nous avons tous de beaux animaux de compagnie.'}]\n"
     ]
    }
   ],
   "source": [
    "# Define the input text for translation\n",
    "text = \"\"\"\\\n",
    "My puppy is adorable, \\\n",
    "Your kitten is cute.\n",
    "Her panda is friendly.\n",
    "His llama is thoughtful. \\\n",
    "We all have nice pets!\"\"\"\n",
    "\n",
    "# Perform translation from English to French using the correct language codes\n",
    "text_translated = translator(text, src_lang=\"eng_Latn\", tgt_lang=\"fra_Latn\")\n",
    "\n",
    "# Output the translated text\n",
    "print(\"Translated text:\", text_translated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fc3232e-1e5e-4072-b285-8d029e4f6034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up resources by deleting the translator model to free up memory\n",
    "del translator\n",
    "\n",
    "# Manually clear the GPU cache if applicable\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Trigger garbage collection to free up any remaining memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c922201-7a40-4715-be69-66001563927b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4daabc908a455f9a71652e1afe3e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325b4d3a30a8478fbcb277c23351e5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b3892e5cee42b4a94cb0d8bead20aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fad063e1ef7438582abcebb8e4f42d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0837c030001944aeba4152b29b0e567f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc191056715419893d2b29747b85551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the BART summarization model from the Hugging Face Model Hub.\n",
    "# Use bfloat16 for GPU (if available) and float32 for CPU.\n",
    "summarizer = pipeline(\n",
    "    task=\"summarization\", \n",
    "    model=\"facebook/bart-large-cnn\", \n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f6a9736-e928-44c5-b4e2-f271e9c0234c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up resources by deleting the summarizer model to free up memory\n",
    "del summarizer\n",
    "\n",
    "# Manually clear the GPU cache if applicable\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Trigger garbage collection to free up any remaining memory\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3bd28a-e46a-4783-9e48-dfe5129d7a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
