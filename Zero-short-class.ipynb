{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd19b058-9761-4986-9872-e6d40612366f",
   "metadata": {},
   "source": [
    "Cell 1: Install necessary libraries (if not already installed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f8894-da47-4290-ac33-ab80d564d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if running on a local machine\n",
    "!pip install transformers  # Hugging Face's Transformers library\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610608b7-9124-43a1-9b71-ac90646f90fd",
   "metadata": {},
   "source": [
    "Cell 2: Suppress warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f4afc-2c23-4452-b62e-24f1815391c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning messages from the Transformers library to keep the output clean.\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72884115-97da-41c2-b78a-6cc5664674d2",
   "metadata": {},
   "source": [
    "Cell 3: Load the CLIP model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f1606-575a-4615-b2a6-5bdc3d7c0af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the CLIP model for zero-shot image classification and the processor to prepare the inputs.\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "\n",
    "# Load the pre-trained CLIP model for image classification from the specified directory.\n",
    "model = CLIPModel.from_pretrained(\"./models/openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Load the processor that formats the image and text (labels) into inputs for the model.\n",
    "processor = AutoProcessor.from_pretrained(\"./models/openai/clip-vit-large-patch14\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b7c03-b897-4345-80ef-2cfc9422f7ae",
   "metadata": {},
   "source": [
    "Cell 4: Load and display the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd79ee79-e414-4494-b8a5-9e6c73a69488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PIL for image loading and processing.\n",
    "from PIL import Image\n",
    "\n",
    "# Load the image from the specified file path. This image will be used for classification.\n",
    "image = Image.open(\"./kittens.jpeg\")\n",
    "\n",
    "# Display the image to verify it's correctly loaded.\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881fddc-c85a-4db0-a15b-d8645477ea9f",
   "metadata": {},
   "source": [
    "Cell 5: Set the list of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52bcf0-d886-4f9d-9f03-8ec25f36cc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of labels that you want the model to classify the image into.\n",
    "# The CLIP model will predict the probability of each label.\n",
    "labels = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "\n",
    "# Use the processor to format the image and text labels as inputs for the model.\n",
    "# The return_tensors=\"pt\" argument ensures that the inputs are returned as PyTorch tensors.\n",
    "inputs = processor(text=labels, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Display the processed inputs for verification.\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d01384-97a3-4e9b-b8b4-767750a0a7ad",
   "metadata": {},
   "source": [
    "Cell 6: Run the CLIP model to get classification outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33791c52-1baf-403b-80ab-c8ec3b7b9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the formatted inputs (image and text labels) through the model to get the classification outputs.\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Display the raw output logits for the image classification task.\n",
    "outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e7f2a7-3e93-420e-ab77-f858bdcfd02b",
   "metadata": {},
   "source": [
    "Cell 7: Extract the logits and calculate probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2e08f-08ce-4604-9257-bef0da177a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the logits (raw scores) for each label with respect to the input image.\n",
    "# logits_per_image represents the model's prediction for the image based on the labels.\n",
    "logits_per_image = outputs.logits_per_image\n",
    "\n",
    "# Apply the softmax function to convert the logits into probabilities, ensuring they sum to 1.\n",
    "# This gives the model's confidence in each label.\n",
    "probs = logits_per_image.softmax(dim=1)[0]\n",
    "\n",
    "# Display the calculated probabilities.\n",
    "probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0885a0-f6ad-48db-b584-78fea2c1f92c",
   "metadata": {},
   "source": [
    "Cell 8: Print the label probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62615e-c7c3-4f10-aeb3-a54f0de40abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the probabilities tensor into a list for easier handling.\n",
    "probs = list(probs)\n",
    "\n",
    "# Loop through each label and print its corresponding probability.\n",
    "for i in range(len(labels)):\n",
    "    print(f\"label: {labels[i]} - probability of {probs[i].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe64e07-0993-46b7-b217-239823562fe7",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "Model and Processor Initialization: We load the CLIP model and processor, which are designed for zero-shot image classification. The model takes both image and text inputs to classify the image based on given text labels.\n",
    "\n",
    "Image Loading: The input image (e.g., kittens.jpeg) is loaded using the PIL library and displayed to ensure correct loading.\n",
    "\n",
    "Label Definition: The labels for classification are defined in natural language (e.g., \"a photo of a cat\", \"a photo of a dog\"). These labels are passed along with the image to the model.\n",
    "\n",
    "Input Processing: The processor formats the image and text labels, converting them into a format that the CLIP model can process. The inputs are returned as PyTorch tensors.\n",
    "\n",
    "Model Inference: The model processes the inputs and returns logits, which represent the raw scores for each label.\n",
    "\n",
    "Probability Calculation: The softmax function is applied to the logits to convert them into probabilities. These probabilities represent the modelâ€™s confidence in each label for the given image.\n",
    "\n",
    "Result Display: The probabilities are printed for each label, showing how likely the model believes the image matches each label.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
